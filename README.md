# ğŸ¤– AI Code Assistant

**AI Code Assistant** is an end-to-end code generation and explanation tool powered by LLMs. It helps developers by autocompleting code, fixing bugs, writing docstrings, and explaining code snippets â€” all through a friendly frontend and a robust backend using modern ML tooling.

---

## ğŸš€ Features

- ğŸ”„ **Autocomplete Code Snippets**
- ğŸ§  **Generate Docstrings**
- ğŸ› ï¸ **Fix Bugs in Code**
- ğŸ’¬ **Explain Code in Plain English**
- ğŸ’» **Interactive Streamlit Frontend**
- âš™ï¸ **FastAPI Backend with Ollama LLM Support**

---

## ğŸ§° Tech Stack

| Component | Technology |
|----------|------------|
| **Frontend** | Streamlit |
| **Backend** | FastAPI |
| **Model Inference** |  Ollama |
| **LLMs Tested** | GPT-2, CodeLlama, StarCoder (small variants) |
| **Prompt Management** | Custom Templates |
| **Python Version** | 3.10+ |
| **Deployment** | Localhost (Free!) |

---

## ğŸ“ Project Structure

<pre> ``` 
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py # FastAPI app and route handlers
â”‚ â”œâ”€â”€ ollama_client.py # Sends prompts to Ollama model
â”‚ â””â”€â”€  prompt_templates.py # Task-specific prompt builders
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ app.py # Streamlit UI for user interaction
â”‚ â””â”€â”€ model_handler.py # Helper to dispatch tasks + route prompts
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation


 ``` </pre>

---

## ğŸ› ï¸ Setup & Run Locally

1. **Clone the repo**  
   ```bash
   git clone https://github.com/SyedAshrafGufran/ai-code-assistant.git
   cd ai-code-assistant

2. Set up virtual environment
   ```bash
   python -m venv venv
   venv\Scripts\activate  # On Windows


3. Install requirements
   ```bash
   pip install -r requirements.txt

4. Start Ollama server (in another terminal)
   ```bash
   ollama run <your-model-name>   # e.g., ollama run codellama


5. Run FastAPI backend
   ```bash
   uvicorn backend.main:app --reload


6. Run Streamlit frontend
   ```bash
   streamlit run frontend/app.py


## ğŸ“Œ Notes

- Works best with lighter LLMs (like **GPT-2** or **CodeLlama 7B**).
- Requires **~4GB+ RAM** minimum if running locally.
- Can be integrated with hosted APIs like **Hugging Face Inference API** if desired.

---

## ğŸ§‘â€ğŸ’» Author

**Syed Ashraf Gufran**  
_Machine Learning & AI Enthusiast_
[LinkedIn](https://www.linkedin.com/in/syed-ashraf-gufran) | [GitHub](https://github.com/SyedAshrafGufran)